{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD6-pr5Nj_lm"
      },
      "source": [
        "# Introduction to JAX: from warp-speed NumPy to neural networks\n",
        "\n",
        "JAX is NumPy on rocket fuel: it runs blazingly fast on CPUs, GPUs, and TPUs by automatically compiling your computational graph to [XLA](https://github.com/openxla/xla). Oh, and it also has autograd and some other cool tricks.\n",
        "\n",
        "**What makes JAX special:**\n",
        "- **Speed**: Your code runs where it's fastest (CPU/GPU/TPU)\n",
        "- **Autograd**: Calculate derivatives of pretty much any function\n",
        "- **Compilation**: Transforms Python into optimized machine code\n",
        "- **Functional core**: Pure functions guarantee reproducibility\n",
        "- **Easy parallelization**: Write code for a single item and run it it over multiple dimensions, data structures, and devices\n",
        "\n",
        "Whether you're training massive language models, solving differential equations, or just trying to make your data science workflow faster, JAX delivers performance without sacrificing readability.\n",
        "\n",
        "**TL;DR**: It's NumPy, but with superpowers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzxTx53-j_lq"
      },
      "source": [
        "## What about PyTorch?\n",
        "\n",
        "### When to use JAX:\n",
        "- When PyTorch is too slow\n",
        "  - Lots of tiny GPU-operations: the compiler will fuse them all together\n",
        "  - Speeding up (large) neural networks, such as [LLMs](https://github.com/xai-org/grok-1)\n",
        "- Projects requiring perfect reproducability and determinism\n",
        "- When working with TPUs *(JAX was literally made for this)*\n",
        "\n",
        "### When to use PyTorch:\n",
        "- Quick & dirty prototyping (i.e., most research code)\n",
        "- When using lots of random numbers that don't need to be reproducable\n",
        "- When you need easy debugging\n",
        "- Projects requiring extensive ecosystem support\n",
        "- Production-grade deep learning (often exported to ONNX)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's get started with JAX\n",
        "First: some handy imports!\n",
        "\n",
        "Notice `jnp = jax.numpy` and `jrand = jax.random`."
      ],
      "metadata": {
        "id": "et3VJcaJxk0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hui9erBRj_lo"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrand\n",
        "from jax import grad, jit, vmap, lax\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Any\n",
        "_counter = 123\n",
        "print(f\"JAX version: {jax.__version__}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZweyhUsj_lp"
      },
      "source": [
        "## JAX = Accelerated NumPy\n",
        "\n",
        "JAX provides a NumPy-like interface that runs on accelerated hardware. Most NumPy operations have direct JAX equivalents.\n",
        "\n",
        "**TL;DR**: Write NumPy code, change `np` into `jnp`, profit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9kSyVHNj_lp"
      },
      "source": [
        "# NumPy array - lives a peaceful life on your CPU\n",
        "np_array = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "# JAX array - same syntax, but secretly lives on your GPU/TPU (if available)\n",
        "jax_array = jnp.array([[1, 2], [3, 4]])\n",
        "\n",
        "# Operations look similar\n",
        "print(\"NumPy sum:\", np_array.sum())\n",
        "print(\"JAX sum:\", jax_array.sum())\n",
        "\n",
        "# But JAX arrays have a special home\n",
        "print(f\"JAX array device: {jax_array.device}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLR9Kbhj_ls"
      },
      "source": [
        "### Automatic device detection in JAX\n",
        "Every 15 seconds, somewhere in the world, a PyTorch dev encounters this error:\\\n",
        "`RuntimeError: Expected all tensors to be on the same device, but found at least two devices`\n",
        "\n",
        "With JAX: never again!\n",
        "\n",
        "JAX automatically picks the 'best' available device for you. It follows a simple priority hierarchy: TPU > GPU > CPU.\\\n",
        "No more need for tedious `.to(device)` calls. Hallelujah!\n",
        "\n",
        "**TL;DR**: JAX handles device placement for you. You can forget it even exists.\n",
        "\n",
        "**Pro Tip:** Always check that JAX detects your GPU, or it'll use the CPU and never tell you. *(ask me how I know)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpadHovbj_ls"
      },
      "source": [
        "print(\"Running JAX on\", jax.lib.xla_bridge.get_backend().platform)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dmLEQuMj_lq"
      },
      "source": [
        "## The foundation of JAX: functional programming\n",
        "In JAX, there cannot be global states. Every function must return the same output when given the same input. This makes the code more predictable and easier to interpret.\n",
        "\n",
        "**TL;DR**: Functions should depend only on their inputs, not on the phase of the moon.\n",
        "\n",
        "**Pro Tip**: In desparate need of a global variable? Provide it as an extra input to the function and return it as an extra output.\n",
        "\\\n",
        "**Pro Tip #2**: Global variables *are* allowed as long as they remain constant throughout the entire program. This can be useful for setting configs and hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFJyILt6j_lq"
      },
      "source": [
        "# Bad: Global state makes your code unpredictable\n",
        "def bad_increment(x):\n",
        "  \"\"\"Global states are like a box of chocolates.\n",
        "  You never know what you're gonna get...\"\"\"\n",
        "  return x + _counter"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Any predictions for what you'll see here?\n",
        "bad_increment(4)"
      ],
      "metadata": {
        "id": "hvbtzNM8rVIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Good: Pure function, like a reliable vending machine\n",
        "def good_increment(x, counter):\n",
        "  return x + counter, counter + 1"
      ],
      "metadata": {
        "id": "T1q_KhHzrTSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Any predictions for what you'll see here?\n",
        "good_increment(4, 1)"
      ],
      "metadata": {
        "id": "eqfV7Ei6sGEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZxLjIilj_lr"
      },
      "source": [
        "### Random numbers without global seeds\n",
        "\n",
        "One global state we all like to ignore, is the **Pseudo-Random Number Generator seed**.\n",
        "\n",
        "Since JAX forbids global states, you need to *manually* set this right! Every single time. Anywhere in your code. Yes, this sucks. But it must be done.\n",
        "\n",
        "**TL;DR**: Random numbers need explicit keys. Always split, never reuse.\n",
        "\n",
        "**Pro Tip**: Create one master key at the start of your program, then split it whenever you need randomness. This keeps your code reproducible."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating random numbers in NumPy\n",
        "Easy peasy, but impossible to predict.\n",
        "Run this cell twice and you'll get different results."
      ],
      "metadata": {
        "id": "KJCq4vdQtqqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_np = np.random.randn(1, 2, 3)\n",
        "print(a_np)"
      ],
      "metadata": {
        "id": "nfhTKCd2tXje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating random numbers in JAX\n",
        "A major hassle, but completely deterministic.\n",
        "Run these cells twice and you'll get the same results."
      ],
      "metadata": {
        "id": "pJr0bgwYt0d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Summon the ancient PRNG key exactly *once*\n",
        "key = jrand.PRNGKey(0)\n",
        "\n",
        "# 2. Never use the ancient key directly! Split it like a coconut\n",
        "key, subkey = jrand.split(key) # = two fresh coconut halves\n",
        "\n",
        "# 3. Use the subkey to generate random numbers\n",
        "a_jax = jrand.normal(subkey, (1, 2, 3))\n",
        "print(a_jax)\n",
        "\n",
        "# 4. Repeat for new random vector\n",
        "key, use_key = jrand.split(key)\n",
        "b_jax = jrand.normal(subkey, (1, 2, 3))\n",
        "print(b_jax)\n",
        "\n",
        "# What definitely NOT to do:\n",
        "subkey = jrand.PRNGKey(0)\n",
        "numbers1 = jrand.normal(subkey, (3,))  # This will always be the same...\n",
        "numbers2 = jrand.normal(subkey, (3,))  # ...as this. Oops!\n",
        "print(numbers1, numbers2)"
      ],
      "metadata": {
        "id": "eyCSzSGeuacV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzVQGln_j_lr"
      },
      "source": [
        "### Array immutability, and how to deal with it\n",
        "In-place array modifications are also a sort of global state operations. Therefore, in JAX, arrays are immutable.\n",
        "\n",
        "**TL;DR**: Don't try to change arrays in-place; create new ones instead.\n",
        "\n",
        "**Pro Tip**: Use `at+set` to create modified copies of arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDBayJY2j_ls"
      },
      "source": [
        "x = jnp.array([1, 2, 3])\n",
        "\n",
        "# This creates a new array\n",
        "y = x.at[0].set(10)\n",
        "\n",
        "print(\"Original array:\", x)\n",
        "print(\"New array:\", y)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjVKojqSj_ls"
      },
      "source": [
        "## JIT Compilation\n",
        "JIT (Just-In-Time) compilation is JAX's secret sauce. It fuses operations together, removes dead or redundant code, and optimizes your code for the available hardware.\\\n",
        "Think of JIT as meal-prepping for the week vs. cooking each meal from scratch — there's an upfront cost, but huge savings afterward.\n",
        "\n",
        "**TL;DR**: JIT makes your code fast, but needs a warmup run. It's worth it for any code you'll run more than once.\n",
        "\n",
        "**Pro Tip**: When timing JAX code, use `.block_until_ready()`. Otherwise, [asynchronous dispatch](https://docs.jax.dev/en/latest/async_dispatch.html) will skew your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eekb0h3j_ls"
      },
      "source": [
        "def coffee_maker(beans):\n",
        "    \"\"\"Simulates a complex morning coffee routine\"\"\"\n",
        "    return jnp.sum(4 * beans + 2 * jnp.sin(beans) ** 2)\n",
        "\n",
        "# First, let's time it without JIT\n",
        "morning_beans = jnp.ones((1000,))\n",
        "print(\"Slooooooowwwww:\")\n",
        "%timeit coffee_maker(morning_beans).block_until_ready()\n",
        "\n",
        "# Now with JIT - first cup is slow (compilation)\n",
        "print(\"Quite slow:\")\n",
        "%timeit jit(coffee_maker)(morning_beans).block_until_ready()\n",
        "\n",
        "# But every cup after that is lightning fast!\n",
        "jitted_coffee = jit(coffee_maker)  # or use @jit if you're fancy\n",
        "_ = jitted_coffee(morning_beans)  # First slow cup\n",
        "print(\"Super fast:\")\n",
        "%timeit jitted_coffee(morning_beans).block_until_ready()  # Fast cups forever"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dmWhkZJj_lq"
      },
      "source": [
        "### Control Flow in JIT\n",
        "\n",
        "Python is loved for its dynamic control flow. But JAX's static compiler doesn't like that. If you want to use `jit`, you'll have to write your code the JAX way.\n",
        "\n",
        "**TL;DR**: Don't use Python's `if/else` and `for/while`. Use their JAX equivalents."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### If / else ⟹ cond / switch / where\n",
        "Use `lax.cond`, `lax.switch` or `jnp.where` instead of `if/else`."
      ],
      "metadata": {
        "id": "OBXRlfK5zU7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cheap coffee on Wednesdays!\n",
        "def discount_bad(day_num, price):\n",
        "    return price * 0.2 if day_num == 3 else 0\n",
        "\n",
        "# This fails with JIT - conditional depends on runtime value\n",
        "try:\n",
        "  jit(discount_bad)(1, 25.0)\n",
        "except jax.errors.TracerBoolConversionError as e:\n",
        "  print(\"BIG ERROR: DON'T USE IF IN JIT\")\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "MTYOO6wHYwhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The JAX way: always calculable paths\n",
        "def discount_good(day_num, price):\n",
        "    return jnp.where(day_num == 3, price * 0.2, 0)\n",
        "\n",
        "# This works, but still no discounts on Mondays :'(\n",
        "jit(discount_good)(1, 25.0)"
      ],
      "metadata": {
        "id": "cvMBodI-aBie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loops ⟹ scan\n",
        "Loops are allowed, but will be unrolled at compilation. This will make the computation graph unnecessarily large and cause compilation times to explode (~4th-order power law). Using `scan` avoids this.\n",
        "\n",
        "That's why DeepMind's slogan is: [\"Always scan when you can!\"](https://github.com/jax-ml/jax/discussions/3850#discussioncomment-44785)"
      ],
      "metadata": {
        "id": "Wr_CSAGYzvTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grind beans with multiple steps\n",
        "def grind_bad(beans):\n",
        "    result = beans\n",
        "    for i in range(4096):  # Yes, I like my beans reduced to dust\n",
        "        result = result * 0.8 + 2  # Each grinding step\n",
        "    return result\n",
        "\n",
        "grind_bad = jit(grind_bad)\n",
        "# For loops work but take long to compile\n",
        "grind_bad(jnp.array([4.0, 8.0]))"
      ],
      "metadata": {
        "id": "g-gywxG1xlLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only the compilation itself is slow. Afterwards, it's super fast.\n",
        "grind_bad(jnp.array([5.0, 7.0]))"
      ],
      "metadata": {
        "id": "mDh1TCJGjmzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Better: Using scan for iterative processes\n",
        "# (but unfortunately it's a bit ugly)\n",
        "def grind_good(beans):\n",
        "    def grind_step(carry, _):\n",
        "        return carry * 0.8 + 2, None\n",
        "    final_brew, _ = lax.scan(grind_step, beans, None, length=4096)\n",
        "    return final_brew\n",
        "\n",
        "grind_good = jit(grind_good)\n",
        "grind_good(jnp.array([4.0, 8.0]))"
      ],
      "metadata": {
        "id": "Z50f8wnD0th7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ironically, fori / scan is a teeny-tiny bit slower than unrolled for-loops\n",
        "grind_good(jnp.array([5.0, 7.0]))"
      ],
      "metadata": {
        "id": "KLe0hkjvkL1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes unrolling loops is actually beneficial! That is because the compiler is not smart enough yet to optimize a scan beyond its boundaries. Unrolling creates a larger computational graph allowing better operator fusion.\n",
        "\n",
        "For example, if your loop starts with `y = x**2` and ends with `x = b**2`, unrolling lets the compiler optimize to `y = b**4`.\n",
        "\n",
        "**Pro Tip**: Use `scan(..., unroll=True)` to get this optimization when needed, though be prepared for slower compilation times."
      ],
      "metadata": {
        "id": "5DtTN2kvlFhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summary\n",
        "\n",
        "| Control Structure | JAX Equivalent | Use When |\n",
        "|------------------|----------------|-----------|\n",
        "| if/else | lax.cond, jnp.where | Simple conditionals |\n",
        "| for loops | lax.scan | Iterative operations |\n",
        "| while loops | lax.while_loop | Dynamic iteration |"
      ],
      "metadata": {
        "id": "K0jHG_LYrE6u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlVRQ9Vgj_ls"
      },
      "source": [
        "## Effortless vectorization with `vmap`\n",
        "PyTorch is very much a \"batch first\" sort of framework. For everything you do, you need to keep into account that there will be an extra batch dimension added to the data.\n",
        "Sometimes, this can be a hassle. If you've ever struggled with broadcasting dimensions in NumPy/PyTorch, you're in for a treat.\n",
        "\n",
        "JAX's `vmap` simplifies batching. Write code for one item, and `vmap` scales it to many — just like that.\n",
        "\n",
        "It's like having a sous-chef that perfectly scales your recipe from serving 1 person to serving 100 without changing how you write the recipe.\n",
        "\n",
        "**TL;DR**: Single-item code, batch-ready with `vmap`.\n",
        "\n",
        "**Pro Tip**: Use `vmap`'s `in_axes` parameter to control which arguments get vectorized. It's like telling your sous \"100 servings, but no need for 100 different pepper mills\".\\\n",
        "**Pro Tip #2**: Abuse the parallel device reduction `pmean` and `psum` to reduce over your vmapped axis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def brew_coffee(beans, water):\n",
        "    \"\"\"Brew a cup of coffee with given beans and water ratio\"\"\"\n",
        "    return jnp.dot(beans, water)\n",
        "\n",
        "# Make a single cup\n",
        "beans = jnp.array([1, 2, 3])  # different bean types (arabica, robusta, etc.)\n",
        "water = jnp.array([0.5, 0.3, 0.2])  # water temperature/pressure ratios\n",
        "print(\"Single cup:\", brew_coffee(beans, water))"
      ],
      "metadata": {
        "id": "_ZJVmku_NnVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say you want to apply a function to a whole batch of inputs.\n",
        "\n",
        "In PyTorch, you'd have to bend your head over the resulting matmul dimensions (or use [einsum](https://pytorch.org/docs/stable/generated/torch.einsum.html)).\n",
        "\n",
        "In JAX, it's as easy as you'd hope:"
      ],
      "metadata": {
        "id": "1450CmcOO4Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make coffee for the whole family\n",
        "batched_brewing = vmap(brew_coffee)\n",
        "family_beans = jnp.stack([beans, beans * 2])  # Some want stronger coffee\n",
        "family_water = jnp.stack([water, water / 2])  # Different water profiles\n",
        "print(\"Family coffee:\", batched_brewing(family_beans, family_water))\n",
        "\n",
        "# Make different coffees with same water settings\n",
        "cafe_brewing = vmap(brew_coffee, in_axes=(0, None))\n",
        "print(\"Different beans, same water:\", cafe_brewing(family_beans, water))"
      ],
      "metadata": {
        "id": "LkynAxU3Pcaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But wait, what if you want to do a reduction after your vmap? (e.g., sum / mean)\n",
        "\n",
        "No worries, JAX has got you covered:"
      ],
      "metadata": {
        "id": "kKUZM_bFRltf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlH6_xuVj_ls"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "# Create a batch of coffee beans and water profiles\n",
        "batch_beans = jnp.array([[1, 2], [3, 4]])\n",
        "batch_water = jnp.array([[0.5, 0.5], [0.7, 0.3]])\n",
        "\n",
        "# To be sure about which axis to reduce over, pmean needs an axis_name arg.\n",
        "my_axis_name: str = \"coffee_batch\"\n",
        "\n",
        "@partial(vmap, axis_name=my_axis_name)\n",
        "def mean_coffee_quality(beans, water):\n",
        "    \"\"\"Calculate mean quality across a batch of coffee brews\"\"\"\n",
        "    brews = brew_coffee(beans, water)\n",
        "    return lax.pmean(brews, axis_name=my_axis_name)\n",
        "\n",
        "print(mean_coffee_quality(batch_beans, batch_water)) # note that the dims were preserved\n",
        "\n",
        "# Adding out_axes=None tells vmap to collapse this unnecessary dim into a scalar\n",
        "@partial(vmap, axis_name=my_axis_name, out_axes=None)\n",
        "def mean_coffee_quality_scalar(beans, water):\n",
        "    \"\"\"Also mean quality, but as a scalar this time\"\"\"\n",
        "    brews = brew_coffee(beans, water)\n",
        "    return lax.pmean(brews, axis_name=my_axis_name)\n",
        "\n",
        "print(mean_coffee_quality_scalar(batch_beans, batch_water)) # collapsed to scalar"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disclaimer:** `pmean` and `psum` are originally meant for reductions over `pmap` operations (hence the `p`). `pmap` **p**arallellizes a big computation over multiple devices. Think of it as having several sous-chefs working together to cook a single huge buffet!"
      ],
      "metadata": {
        "id": "IRWqNlwH1xuY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Ddzv82j_lt"
      },
      "source": [
        "## Automatic Differentiation with `grad`\n",
        "\n",
        "JAX makes calculating derivatives as easy as making a cup of coffee. Actually, it's much easier: JAX does all the work for you! Just wrap your function with `grad` and you're done.\n",
        "\n",
        "**TL;DR**: `grad` turns any function into its derivative. No calculus required!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK3exLwyj_lt"
      },
      "source": [
        "# Let's differentiate a coffee pricing model\n",
        "def coffee_price(beans, milk):\n",
        "    \"\"\"Compute price of a coffee based on ingredients\"\"\"\n",
        "    return 0.5 * beans**2 + 2.0 * milk**2  # Fancy quadratic pricing\n",
        "\n",
        "# Get derivative wrt beans\n",
        "d_beans = grad(coffee_price)  # Default: differentiate wrt first arg\n",
        "print(\"If beans go up by $1, coffee price changes by $\",\n",
        "      d_beans(5.0, 3.0))  # Evaluate at beans=$5, milk=$3\n",
        "\n",
        "# Get both derivatives at once\n",
        "d_both = grad(coffee_price, argnums=(0,1))\n",
        "print(\"Price sensitivity to ingredients:\", d_both(5.0, 3.0))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Power combo: stacking transformations\n",
        "\n",
        "One of JAX's most powerful features is how seamlessly you can combine its fundamental building blocks. Like LEGOs, you can stack `jit`, `vmap`, `grad`, and more to create complex functions with minimal code.\n",
        "\n",
        "**Pro Tip:** use `@` decorators above a function definition to keep your code clean."
      ],
      "metadata": {
        "id": "oAys1HrAPnf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do a fancy market analysis\n",
        "\n",
        "# Stack transformations to analyze price elasticity\n",
        "@jit  # Make it fast\n",
        "@vmap  # Apply to multiple prices at once\n",
        "@grad  # Get price sensitivity\n",
        "def profit_sensitivity(beans_price):\n",
        "    \"\"\"How sensitive is profit to bean price (relative to milk price)?\"\"\"\n",
        "    return coffee_price(beans_price, jnp.ones_like(beans_price))\n",
        "\n",
        "# Analyze for 5 different bean costs\n",
        "bean_prices = jnp.array([4.0, 5.0, 6.0, 7.0, 8.0])\n",
        "sensitivity = profit_sensitivity(bean_prices)\n",
        "print(f\"Profit sensitivity to bean prices: {sensitivity}\")\n",
        "\n",
        "# Want second derivatives? Just add another grad!\n",
        "@jit\n",
        "@vmap\n",
        "@grad\n",
        "@grad\n",
        "def profit_elasticity(beans_price):\n",
        "    \"\"\"How quickly does sensitivity change with price?\"\"\"\n",
        "    return coffee_price(beans_price, jnp.ones_like(beans_price))\n",
        "\n",
        "elasticity = profit_elasticity(bean_prices)\n",
        "print(f\"Profit elasticity: {elasticity}\")"
      ],
      "metadata": {
        "id": "L7291nEsQH2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozrr4HtUj_lt"
      },
      "source": [
        "## PyTrees: Python data structures on steroids\n",
        "\n",
        "PyTrees are JAX's way of handling complex data structures. Think of them as Python containers (dicts, lists, tuples) that JAX can magically understand and operate on.\n",
        "\n",
        "**TL;DR**: PyTrees let you use JAX with structured data, not just arrays.\n",
        "\n",
        "**Pro Tip**: Use `jax.tree.map` to apply functions to every array in a PyTree. It's like `vmap`, but for nested structures instead of batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWg9-lprj_lt"
      },
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "class CoffeeOrder(NamedTuple):\n",
        "    \"\"\"A very sophisticated coffee order\"\"\"\n",
        "    drinks: dict[str, list[tuple[jax.Array, jax.Array]]]  # (beans, milk) per cup\n",
        "    extras: tuple[dict[str, float]]  # sugar, cinnamon, etc.\n",
        "\n",
        "# A complex order for a very picky coffee shop\n",
        "my_coffee_order = CoffeeOrder(\n",
        "    drinks = {\n",
        "        \"espresso\": [(jnp.array([20.0]), jnp.array([0.0]))],  # No milk!\n",
        "        \"latte\": [(jnp.array([15.0]), jnp.array([150.0]))],  # Lots of milk\n",
        "    },\n",
        "    extras = ({\"sugar\": 2.0, \"cinnamon\": 0.5},)\n",
        ")\n",
        "print(\"Your complex order:\", my_coffee_order)\n",
        "\n",
        "# Calculate prices for everything in one go\n",
        "def apply_coffee_price(bean_milk_tuple):\n",
        "    beans, milk = bean_milk_tuple\n",
        "    return coffee_price(beans, milk)\n",
        "\n",
        "# Get prices for the drinks only (I refuse to pay for extras)\n",
        "# Also, tell tree.map to expect a tuple as input (default: jax.Array)\n",
        "print(\"Prices:\", jax.tree.map(apply_coffee_price,\n",
        "                              my_coffee_order.drinks,\n",
        "                              is_leaf=lambda x: isinstance(x, tuple)))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This feature is incredibly useful. You can define a function on a simple array and expect it to extrapolate to arbitrary PyTrees. Powerful stuff!\n",
        "\n",
        "Doing this in PyTorch would probably require a whole bunch of nested for loops."
      ],
      "metadata": {
        "id": "FkATQJWyf3fB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPKn6Htdj_lt"
      },
      "source": [
        "## Neural Networks with Equinox\n",
        "\n",
        "JAX is at its strongest we have some big computation repeating itself for many iterations. Oh, and also if we need gradients and batches.\n",
        "\n",
        "So obviously, there's one great application for JAX: **neural networks**\n",
        "\n",
        "But there's a problem. Remember how JAX hates state? Well, neural networks are basically big bags of state (weights, biases, batch statistics...).\n",
        "\n",
        "This is where [Equinox](https://github.com/patrick-kidger/equinox) comes in: it converts all that state into a *single* PyTree that JAX can handle.\n",
        "\n",
        "**TL;DR:** Use Equinox when you want PyTorch-like simplicity with JAX-like speed.\n",
        "\n",
        "**Pro Tip:** Use Equinox's `filter_{jit,grad,vmap}` instead of the JAX building blocks. It'll save you some headaches."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps equinox jaxtyping"
      ],
      "metadata": {
        "id": "3UnRN91zHAdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cOD7Ynlj_lt"
      },
      "source": [
        "import equinox as eqx\n",
        "\n",
        "class MLP(eqx.Module):\n",
        "    \"\"\"A PyTree pretending to be a Multi-Layer Perceptron\"\"\"\n",
        "    layers: list\n",
        "\n",
        "    def __init__(self, key):\n",
        "        # Split our key into three (like a responsible adult)\n",
        "        keys = jrand.split(key, 3)\n",
        "\n",
        "        # Use keys for random weight init\n",
        "        self.layers = [\n",
        "            eqx.nn.Linear(784, 512, key=keys[0]),  # Big layer\n",
        "            eqx.nn.Linear(512, 256, key=keys[1]),  # Medium layer\n",
        "            eqx.nn.Linear(256, 10, key=keys[2])    # Small layer\n",
        "        ]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Feed forward, with extra GELU sauce\n",
        "        for i, layer in enumerate(self.layers[:-1]):\n",
        "            x = jax.nn.gelu(layer(x))\n",
        "        return self.layers[-1](x)  # Final layer, no activation\n",
        "\n",
        "# Let's see what this PyTree looks like\n",
        "model = MLP(jrand.PRNGKey(42))  # 42 is the answer to everything\n",
        "print(\"Your model's family tree:\", model)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p11ks-VZj_lt"
      },
      "source": [
        "## MNIST Training Example\n",
        "Alright, let's put everything together and train an MLP on MNIST!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "**Plot twist:** JAX has no official data loading library.\n",
        "\n",
        "You have to load *another* Deep Learning library, just for data :'("
      ],
      "metadata": {
        "id": "Z3H5-LCaHY_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You'll need to borrow PyTorch's dataloaders (yes, this takes a while to load)\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_mnist(batch_size=64):\n",
        "    # Normalize and flatten\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,)),\n",
        "        transforms.Lambda(lambda x: x.reshape(-1)),\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "6BFmVdjbHQ42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Useful training functions\n",
        "**Pro Tip**: With nested functions, only the outermost JIT counts. Any other JITs get ignored."
      ],
      "metadata": {
        "id": "Nj4cYQZ6-xo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Not an outer function, so no need for JIT here (though it couldn't hurt)\n",
        "def loss_fn(model, x, y):\n",
        "    \"\"\"A Cross-Entropy Loss that needs to go down\"\"\"\n",
        "    pred = vmap(model)(x)\n",
        "    y_onehot = jax.nn.one_hot(y, num_classes=10)\n",
        "    return jnp.mean(optax.softmax_cross_entropy(pred, y_onehot))\n",
        "\n",
        "@eqx.filter_jit()  # We want this crazy fast!\n",
        "def update_step(model, optimizer, opt_state, x, y):\n",
        "    \"\"\"You give me model, I give you better model\"\"\"\n",
        "    grads = eqx.filter_grad(loss_fn)(model, x, y)  # Only grads for first arg\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    model = eqx.apply_updates(model, updates)\n",
        "    return model, opt_state"
      ],
      "metadata": {
        "id": "6EhcrE9Q-63A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Useful evaluation functions"
      ],
      "metadata": {
        "id": "pBhXFOJ8_FvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@eqx.filter_jit()\n",
        "def get_loss_and_acc(model, x, y):\n",
        "    \"\"\"Combined loss+acc calculation for speedy JIT\"\"\"\n",
        "    pred = vmap(model)(x)  # JIT is smart enough to reuse this in loss_fn\n",
        "    loss = loss_fn(model, x, y)\n",
        "    acc = jnp.mean(jnp.argmax(pred, axis=1) == y)\n",
        "    return loss, acc\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    \"\"\"Check how good a model is on a given dataset\"\"\"\n",
        "    total_loss, total_acc, count = 0.0, 0.0, 0\n",
        "\n",
        "    for data, target in data_loader:\n",
        "        x, y = jnp.array(data.numpy()), jnp.array(target.numpy())\n",
        "\n",
        "        loss, acc = get_loss_and_acc(model, x, y)\n",
        "\n",
        "        # Accumulate metrics\n",
        "        total_loss += loss\n",
        "        total_acc += acc\n",
        "        count += 1\n",
        "\n",
        "    return total_loss/count, total_acc/count"
      ],
      "metadata": {
        "id": "h-ZmeIC7_Jiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "NrrYGguJKIKN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUSBk7BSj_lt"
      },
      "source": [
        "import optax\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_mnist(key, num_epochs=3):\n",
        "    # Setup\n",
        "    model = MLP(key)\n",
        "    optimizer = optax.adam(1e-3)\n",
        "    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
        "    train_loader, test_loader = load_mnist()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train\n",
        "        for data, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            x, y = jnp.array(data.numpy()), jnp.array(target.numpy())\n",
        "            model, opt_state = update_step(model, optimizer, opt_state, x, y)\n",
        "\n",
        "        # Evaluate\n",
        "        _, train_acc = evaluate(model, train_loader)\n",
        "        _, test_acc = evaluate(model, test_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train acc={train_acc:.4f}, Test acc={test_acc:.4f}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghvd91DGj_lu"
      },
      "source": [
        "### Running the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VF80hULj_lu"
      },
      "source": [
        "# Step 1: Choose an initial key (the responsible thing to do, remember?)\n",
        "key = jrand.PRNGKey(0)\n",
        "\n",
        "# Step 2: Train the model (AKA coffee break time!)\n",
        "model = train_mnist(key)\n",
        "\n",
        "# Step 3: World domination"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping up: how to master JAX\n",
        "\n",
        "Together, we've explored the basics of JAX, from random keys to training a neural network.\n",
        "JAX's learning curve might feel steep at first, but the performance gains are worth it once you get comfortable with the paradigm shift.\n",
        "\n",
        "**So, what's next?**\n",
        "- **Get a coffee:** You've earned it. #priorities\n",
        "- **Practice the JAX mindset:** Functional programming with immutable data takes time to internalize. Start by converting some PyTorch scripts to JAX using your favorite LLM, and play around with the possiblities.\n",
        "- **Master the key transformations:** The `jit`, `grad`, `vmap` trio is insanely powerful. Once you get used to it, you can't live without.\n",
        "- **Explore the ecosystem:**\n",
        "  - **[Equinox](https://github.com/patrick-kidger/equinox)** (obviously)\n",
        "  - **[Optax](https://github.com/deepmind/optax)** for optimizers and loss functions\n",
        "  - **[Diffrax](https://github.com/patrick-kidger/diffrax)** for differential equations\n",
        "  - **[Jumanji](https://github.com/instadeepai/jumanji)** for reinforcement learning\n",
        "  - **[And many many more...](https://github.com/n2cholas/awesome-jax)**\n",
        "\n",
        "**Common pitfalls to avoid:**\n",
        "- Reusing PRNG keys (leads to identical \"random\" numbers)\n",
        "- In-place mutations (doesn't work on JAX arrays)\n",
        "- Relying on Python control flow inside JIT (use JAX-specific alternatives)\n",
        "- Expecting JIT to speed up single-use functions (first run is always slow)\n",
        "\n",
        "**Remember:** JAX shines brightest on complex, performance-critical tasks. For quick prototyping, PyTorch might still be your best friend. Choose the right tool for your job!\n",
        "\n",
        "**Fall in love:** Once you get to know JAX and see your first 10x speedup, you'll understand why it's worth the effort. Happy JAXing!"
      ],
      "metadata": {
        "id": "kqjTVmj3v47K"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}